#!/usr/bin/env python3
"""
multimodal_bot_detection.py

Experiments:
1) Text-only (DistilRoBERTa)
2) Image-only (CLIP ViT-B/32 vision branch)
3) Multimodal - Concatenation
4) Multimodal - GMU
5) Multimodal - Cross-Modal Attention (text seq <-> image patch tokens)

Backbone encoders (consistent across all experiments):
- Text:    distilroberta-base  (hidden_size = 768)
- Image:   openai/clip-vit-base-patch32 (vision hidden_size = 768)

Exports (under --out-dir):
- results.json   (combined metrics & training curves for all models)
- results.csv    (combined metrics table for all models)
- test_preds_{model}.npz  (y_true, y_prob, y_pred)
- embeddings_{split}_{model}.pt  (image_embs, text_embs, labels, has_bio, image_missing)
"""

import os
import json
import time
import argparse
import hashlib
import warnings
from typing import Dict, Any, Tuple, List

warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
from PIL import Image
from tqdm import tqdm

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    roc_auc_score,
    average_precision_score,
    precision_recall_fscore_support,
    confusion_matrix,
)

from transformers import (
    AutoTokenizer,
    AutoModel,
    CLIPVisionModel,
    CLIPImageProcessor,
)

# -------------------------------------------------------
# Utilities: image cache naming and safe image load
# -------------------------------------------------------
def url_to_cache_path(url: str, cache_dir: str) -> str:
    h = hashlib.sha1(url.encode("utf-8")).hexdigest()
    return os.path.join(cache_dir, h + ".jpg")

def fetch_image_from_cache_or_placeholder(ref: str, cache_dir: str) -> Image.Image:
    """Return PIL image if cached/exists; otherwise a black placeholder. No network."""
    try:
        if isinstance(ref, str) and ref.lower().startswith(("http://", "https://")):
            p = url_to_cache_path(ref, cache_dir)
            if os.path.exists(p):
                return Image.open(p).convert("RGB")
            return Image.new("RGB", (224, 224), (0, 0, 0))
        else:
            if os.path.exists(str(ref)):
                return Image.open(str(ref)).convert("RGB")
            return Image.new("RGB", (224, 224), (0, 0, 0))
    except Exception:
        return Image.new("RGB", (224, 224), (0, 0, 0))

def exists_in_cache_or_fs(ref: str, cache_dir: str) -> int:
    """0 if present, 1 if missing (for image_missing flag)."""
    try:
        if isinstance(ref, str) and ref.lower().startswith(("http://", "https://")):
            return 0 if os.path.exists(url_to_cache_path(ref, cache_dir)) else 1
        return 0 if os.path.exists(str(ref)) else 1
    except Exception:
        return 1

# -------------------------------------------------------
# Dataset
# -------------------------------------------------------
class BotDetectionDataset(Dataset):
    def __init__(
        self,
        df: pd.DataFrame,
        image_col: str,
        bio_col: str,
        label_col: str,
        image_cache: str,
    ):
        self.df = df.reset_index(drop=True)
        self.image_col = image_col
        self.bio_col = bio_col
        self.label_col = label_col
        self.image_cache = image_cache

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        r = self.df.iloc[idx]
        bio = r[self.bio_col]
        if pd.isna(bio) or str(bio).strip() == "":
            bio = "<NO_BIO>"
        img_ref = r[self.image_col]
        label = int(r[self.label_col])

        image_missing = exists_in_cache_or_fs(img_ref, self.image_cache)
        image = fetch_image_from_cache_or_placeholder(img_ref, self.image_cache)

        return {
            "bio": bio,
            "image": image,
            "label": label,
            "has_bio": 0 if bio == "<NO_BIO>" else 1,
            "image_missing": int(image_missing),
            "img_ref": img_ref,
        }

def collate_batch(
    batch: List[Dict[str, Any]],
    tokenizer,
    image_processor,
    max_length: int,
) -> Dict[str, Any]:
    """Keep EVERYTHING on CPU here. Move to GPU later inside the loops."""
    images = [b["image"] for b in batch]
    bios   = [b["bio"] for b in batch]

    labels = torch.tensor([b["label"] for b in batch], dtype=torch.long)          # CPU
    has_bio = torch.tensor([b["has_bio"] for b in batch], dtype=torch.float32)    # CPU
    image_missing = torch.tensor([b["image_missing"] for b in batch], dtype=torch.float32)  # CPU

    pixel_values = image_processor(images=images, return_tensors="pt")["pixel_values"]  # CPU
    tokenized = tokenizer(
        bios, padding=True, truncation=True, max_length=max_length, return_tensors="pt"
    )  # CPU

    return {
        "pixel_values": pixel_values,              # (B, 3, H, W) CPU
        "input_ids": tokenized["input_ids"],       # CPU
        "attention_mask": tokenized["attention_mask"],  # CPU
        "labels": labels,                          # CPU
        "has_bio": has_bio,                        # CPU
        "image_missing": image_missing,            # CPU
    }

# -------------------------------------------------------
# Backbones
# -------------------------------------------------------
def get_text_backbone(device: str):
    text_name = "distilroberta-base"
    tok = AutoTokenizer.from_pretrained(text_name)
    model = AutoModel.from_pretrained(text_name).to(device)
    hidden = model.config.hidden_size  # 768
    return tok, model, hidden

def get_vision_backbone(device: str):
    vis_name = "openai/clip-vit-base-patch32"
    image_processor = CLIPImageProcessor.from_pretrained(vis_name)
    vision_model = CLIPVisionModel.from_pretrained(
        vis_name,
        use_safetensors=True,
        dtype=torch.float32,   # avoid torch_dtype deprecation warnings
    ).to(device)
    hidden = vision_model.config.hidden_size  # 768
    return image_processor, vision_model, hidden

# -------------------------------------------------------
# Heads / Multimodal models
# -------------------------------------------------------
class TextOnly(nn.Module):
    def __init__(self, text_model: AutoModel, hidden: int = 768):
        super().__init__()
        self.text = text_model
        self.classifier = nn.Sequential(
            nn.Linear(hidden, 256), nn.ReLU(), nn.Dropout(0.3),
            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.3),
            nn.Linear(128, 2)
        )

    def forward(self, input_ids, attention_mask, **_):
        out = self.text(input_ids=input_ids, attention_mask=attention_mask)
        cls = out.last_hidden_state[:, 0, :]  # [CLS]/first token
        return self.classifier(cls)

class ImageOnly(nn.Module):
    def __init__(self, vision_model: CLIPVisionModel, hidden: int = 768):
        super().__init__()
        self.vision = vision_model
        self.missing_img_emb = nn.Parameter(torch.randn(hidden) * 0.02)
        self.classifier = nn.Sequential(
            nn.Linear(hidden, 512), nn.ReLU(), nn.Dropout(0.3),
            nn.Linear(512, 128), nn.ReLU(), nn.Dropout(0.3),
            nn.Linear(128, 2)
        )

    def forward(self, pixel_values, image_missing, **_):
        vis_out = self.vision(pixel_values=pixel_values).last_hidden_state
        img_cls = vis_out[:, 0, :]  # (B, 768)
        miss = image_missing.unsqueeze(1)  # (B,1)
        missing_vec = self.missing_img_emb.unsqueeze(0)  # (1,768)
        img_replaced = img_cls * (1.0 - miss) + missing_vec * miss
        return self.classifier(img_replaced)

class ConcatFusion(nn.Module):
    def __init__(self, text_model: AutoModel, vision_model: CLIPVisionModel, hidden: int = 768):
        super().__init__()
        self.text = text_model
        self.vision = vision_model
        self.missing_img_emb = nn.Parameter(torch.randn(hidden) * 0.02)
        self.fuse = nn.Linear(hidden * 2, hidden)
        self.classifier = nn.Sequential(
            nn.Linear(hidden, 256), nn.ReLU(), nn.Dropout(0.3),
            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.3),
            nn.Linear(128, 2)
        )

    def forward(self, input_ids, attention_mask, pixel_values, image_missing, **_):
        t = self.text(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]
        v_tokens = self.vision(pixel_values=pixel_values).last_hidden_state
        v = v_tokens[:, 0, :]
        miss = image_missing.unsqueeze(1)
        v = v * (1.0 - miss) + self.missing_img_emb.unsqueeze(0) * miss
        fused = self.fuse(torch.cat([t, v], dim=1))
        return self.classifier(fused)

class GMUFusion(nn.Module):
    def __init__(self, text_model: AutoModel, vision_model: CLIPVisionModel, hidden: int = 768):
        super().__init__()
        self.text = text_model
        self.vision = vision_model
        self.missing_img_emb = nn.Parameter(torch.randn(hidden) * 0.02)
        self.text_proj = nn.Linear(hidden, hidden)
        self.image_proj = nn.Linear(hidden, hidden)
        self.gate = nn.Sequential(nn.Linear(hidden * 2, hidden), nn.Sigmoid())
        self.classifier = nn.Sequential(
            nn.Linear(hidden, 256), nn.ReLU(), nn.Dropout(0.3),
            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.3),
            nn.Linear(128, 2)
        )

    def forward(self, input_ids, attention_mask, pixel_values, image_missing, **_):
        t_raw = self.text(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]
        v_tokens = self.vision(pixel_values=pixel_values).last_hidden_state
        v_raw = v_tokens[:, 0, :]
        miss = image_missing.unsqueeze(1)
        v_raw = v_raw * (1.0 - miss) + self.missing_img_emb.unsqueeze(0) * miss

        t = torch.tanh(self.text_proj(t_raw))
        v = torch.tanh(self.image_proj(v_raw))
        z = self.gate(torch.cat([t_raw, v_raw], dim=1))
        h = z * t + (1 - z) * v
        return self.classifier(h)

class CrossAttentionFusion(nn.Module):
    def __init__(self, text_model: AutoModel, vision_model: CLIPVisionModel, hidden: int = 768, num_heads: int = 8):
        super().__init__()
        self.text = text_model
        self.vision = vision_model
        self.missing_img_emb = nn.Parameter(torch.randn(hidden) * 0.02)

        self.text_proj = nn.Linear(hidden, hidden)
        self.image_proj = nn.Linear(hidden, hidden)

        self.t2i = nn.MultiheadAttention(embed_dim=hidden, num_heads=num_heads, batch_first=True)
        self.i2t = nn.MultiheadAttention(embed_dim=hidden, num_heads=num_heads, batch_first=True)

        self.fuse = nn.Linear(hidden * 2, hidden)
        self.classifier = nn.Sequential(
            nn.Linear(hidden, 256), nn.ReLU(), nn.Dropout(0.3),
            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.3),
            nn.Linear(128, 2)
        )

    def forward(self, input_ids, attention_mask, pixel_values, image_missing, **_):
        t_seq = self.text(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state  # (B,Lt,768)
        i_seq = self.vision(pixel_values=pixel_values).last_hidden_state  # (B,Li,768)

        miss = image_missing.view(-1, 1, 1)  # (B,1,1)
        missing_tok = self.missing_img_emb.view(1, 1, -1)  # (1,1,768)
        i_seq = i_seq * (1.0 - miss) + missing_tok * miss

        t_proj = self.text_proj(t_seq)
        i_proj = self.image_proj(i_seq)

        t_att, _ = self.t2i(t_proj, i_proj, i_proj)
        i_query = i_proj.mean(dim=1, keepdim=True)
        i_att, _ = self.i2t(i_query, t_proj, t_proj)

        t_pool = t_att[:, 0, :]
        i_pool = i_att[:, 0, :]

        fused = self.fuse(torch.cat([t_pool, i_pool], dim=1))
        return self.classifier(fused)

# -------------------------------------------------------
# Metrics / eval helpers
# -------------------------------------------------------
def compute_metrics_from_logits(y_true: np.ndarray, logits: np.ndarray) -> Dict[str, Any]:
    probs = 1.0 / (1.0 + np.exp(-logits))
    preds = (probs >= 0.5).astype(int)
    try:
        auc = roc_auc_score(y_true, probs)
    except Exception:
        auc = float("nan")
    try:
        pr = average_precision_score(y_true, probs)
    except Exception:
        pr = float("nan")
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, preds, average="binary", zero_division=0)
    tn, fp, fn, tp = confusion_matrix(y_true, preds).ravel()
    return {
        "auc": float(auc),
        "pr": float(pr),
        "precision": float(precision),
        "recall": float(recall),
        "f1": float(f1),
        "tp": int(tp),
        "fp": int(fp),
        "tn": int(tn),
        "fn": int(fn),
        "y_pred": preds,
        "y_prob": probs,
    }

@torch.no_grad()
def evaluate_dl(model, loader, device) -> Tuple[Dict[str, Any], np.ndarray, np.ndarray, np.ndarray]:
    model.eval()
    logits_list = []
    labels_list = []
    for batch in tqdm(loader, desc="Evaluating"):
        batch = {k: (v.to(device, non_blocking=True) if torch.is_tensor(v) else v) for k, v in batch.items()}
        logits = model(
            input_ids=batch.get("input_ids"),
            attention_mask=batch.get("attention_mask"),
            pixel_values=batch.get("pixel_values"),
            image_missing=batch.get("image_missing"),
            has_bio=batch.get("has_bio"),
        )
        labels_list.append(batch["labels"].cpu().numpy())
        if logits.ndim == 2 and logits.shape[1] == 2:
            logit_pos = (logits[:, 1] - logits[:, 0]).detach().cpu().numpy()
            logits_list.append(logit_pos)
        else:
            logits_list.append(logits.detach().cpu().numpy())
    logits = np.concatenate(logits_list, axis=0)
    y_true = np.concatenate(labels_list, axis=0)
    metrics = compute_metrics_from_logits(y_true, logits)
    return metrics, y_true, metrics["y_prob"], metrics["y_pred"]

def export_preds(out_dir: str, model_key: str, y_true, y_prob, y_pred):
    np.savez(os.path.join(out_dir, f"test_preds_{model_key}.npz"),
             y_true=y_true, y_prob=y_prob, y_pred=y_pred)

def export_embeddings(out_dir: str, split: str, model_key: str,
                      image_embs: np.ndarray, text_embs: np.ndarray,
                      labels: np.ndarray, has_bio: np.ndarray, image_missing: np.ndarray):
    torch.save({
        "image_embs": image_embs,
        "text_embs": text_embs,
        "labels": labels,
        "has_bio": has_bio,
        "image_missing": image_missing,
    }, os.path.join(out_dir, f"embeddings_{split}_{model_key}.pt"))

# -------------------------------------------------------
# Embedding extraction
# -------------------------------------------------------
@torch.no_grad()
def extract_simple_embeddings(
    loader: DataLoader,
    text_model: AutoModel,
    vision_model: CLIPVisionModel,
    device: str
):
    text_model.eval()
    vision_model.eval()
    image_embs = []
    text_embs = []
    labels = []
    has_bio = []
    image_missing = []

    for batch in tqdm(loader, desc="Embeddings"):
        batch = {k: (v.to(device, non_blocking=True) if torch.is_tensor(v) else v) for k, v in batch.items()}
        t_out = text_model(input_ids=batch["input_ids"], attention_mask=batch["attention_mask"], output_hidden_states=False, return_dict=True)
        last_hidden = t_out.last_hidden_state  # (B,L,768)
        att_mask = batch["attention_mask"].unsqueeze(-1).to(last_hidden.dtype)  # (B,L,1)
        pooled = (last_hidden * att_mask).sum(dim=1) / att_mask.sum(dim=1).clamp(min=1.0)  # (B,768)
        text_embs.append(pooled.detach().cpu())

        v_tokens = vision_model(pixel_values=batch["pixel_values"]).last_hidden_state  # (B,Li,768)
        v_cls = v_tokens[:, 0, :]  # (B,768)
        image_embs.append(v_cls.detach().cpu())

        labels.append(batch["labels"].cpu())
        has_bio.append(batch["has_bio"].cpu())
        image_missing.append(batch["image_missing"].cpu())

    image_embs = torch.cat(image_embs, dim=0).numpy()
    text_embs  = torch.cat(text_embs, dim=0).numpy()
    labels     = torch.cat(labels, dim=0).numpy()
    has_bio    = torch.cat(has_bio, dim=0).numpy()
    image_missing = torch.cat(image_missing, dim=0).numpy()
    return image_embs, text_embs, labels, has_bio, image_missing

# -------------------------------------------------------
# Training loop with curves
# -------------------------------------------------------
def train_model(
    model: nn.Module,
    train_loader: DataLoader,
    val_loader: DataLoader,
    device: str,
    epochs: int,
    lr: float,
    model_key: str
):
    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)
    criterion = nn.CrossEntropyLoss()
    best_state = None
    best_val_auc = -1.0
    curves = {"epoch": [], "loss": [], "val_auc": []}

    for ep in range(1, epochs + 1):
        model.train()
        total_loss = 0.0
        n = 0
        for batch in tqdm(train_loader, desc=f"Training [{model_key}]"):
            batch = {k: (v.to(device, non_blocking=True) if torch.is_tensor(v) else v) for k, v in batch.items()}
            logits = model(
                input_ids=batch.get("input_ids"),
                attention_mask=batch.get("attention_mask"),
                pixel_values=batch.get("pixel_values"),
                image_missing=batch.get("image_missing"),
                has_bio=batch.get("has_bio"),
            )
            loss = criterion(logits, batch["labels"])
            opt.zero_grad()
            loss.backward()
            opt.step()
            total_loss += loss.item() * logits.size(0)
            n += logits.size(0)

        avg_loss = total_loss / max(1, n)
        val_metrics, _, _, _ = evaluate_dl(model, val_loader, device)
        val_auc = val_metrics["auc"]

        curves["epoch"].append(ep)
        curves["loss"].append(float(avg_loss))
        curves["val_auc"].append(float(val_auc))

        if val_auc > best_val_auc:
            best_val_auc = val_auc
            best_state = model.state_dict()

    if best_state is not None:
        model.load_state_dict(best_state)
    return model, curves

# -------------------------------------------------------
# Main
# -------------------------------------------------------
def main():
    ap = argparse.ArgumentParser(description="Multimodal Bot Detection with CLIP (vision) + DistilRoBERTa (text)")
    ap.add_argument("--csv", required=True)
    ap.add_argument("--image-col", default="profile_image_url")
    ap.add_argument("--bio-col", default="description")
    ap.add_argument("--label-col", default="account_type")
    ap.add_argument("--image-cache", default="image_cache")
    ap.add_argument("--out-dir", default="outputs")
    ap.add_argument("--batch-size", type=int, default=32)
    ap.add_argument("--epochs", type=int, default=30)
    ap.add_argument("--seed", type=int, default=42)
    ap.add_argument("--device", default=None)  # cuda/mps/cpu
    ap.add_argument("--max-length", type=int, default=96)
    ap.add_argument("--lr", type=float, default=1e-4)
    ap.add_argument("--skip-text-only", action="store_true")
    ap.add_argument("--skip-image-only", action="store_true")
    args = ap.parse_args()

    # Device
    if args.device is None:
        if torch.cuda.is_available():
            device = "cuda"
        elif torch.backends.mps.is_available():
            device = "mps"
        else:
            device = "cpu"
    else:
        device = args.device
    print(f"Using device: {device}")

    # Seeds
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)

    os.makedirs(args.out_dir, exist_ok=True)
    os.makedirs(args.image_cache, exist_ok=True)

    # Load CSV
    df = pd.read_csv(args.csv)
    if args.label_col not in df.columns or args.image_col not in df.columns or args.bio_col not in df.columns:
        raise ValueError("CSV must contain the specified --label-col, --image-col, and --bio-col")

    # Map labels if strings
    if df[args.label_col].dtype == object or df[args.label_col].dtype == "O":
        df[args.label_col] = df[args.label_col].map({"bot": 1, "human": 0})

    # Split
    if "split" in df.columns:
        train_df = df[df["split"] == "train"].drop(columns=["split"])
        val_df   = df[df["split"] == "val"].drop(columns=["split"])
        test_df  = df[df["split"] == "test"].drop(columns=["split"])
    else:
        trainval_df, test_df = train_test_split(df, test_size=0.10, random_state=args.seed, stratify=df[args.label_col])
        train_df, val_df = train_test_split(trainval_df, test_size=0.20, random_state=args.seed, stratify=trainval_df[args.label_col])

    print(f"Split sizes: train={len(train_df)}, val={len(val_df)}, test={len(test_df)}")

    # Backbones
    tokenizer, text_model, text_hidden = get_text_backbone(device)
    image_processor, vision_model, vision_hidden = get_vision_backbone(device)
    assert text_hidden == vision_hidden == 768, "Expected 768-dim hidden for both backbones."

    # Datasets / Loaders (pin_memory only helps if tensors are on CPU â€” which they are now)
    def make_loader(split_df, shuffle: bool):
        ds = BotDetectionDataset(split_df, args.image_col, args.bio_col, args.label_col, args.image_cache)
        return DataLoader(
            ds,
            batch_size=args.batch_size,
            shuffle=shuffle,
            num_workers=0,                         # keep 0 to avoid fork CUDA issues
            pin_memory=(device == "cuda"),
            collate_fn=lambda b: collate_batch(b, tokenizer, image_processor, args.max_length),
        )

    train_loader = make_loader(train_df, shuffle=True)
    val_loader   = make_loader(val_df, shuffle=False)
    test_loader  = make_loader(test_df, shuffle=False)

    # Fresh backbones for each experiment
    def fresh_text():
        return AutoModel.from_pretrained("distilroberta-base").to(device)

    def fresh_vision():
        return CLIPVisionModel.from_pretrained(
            "openai/clip-vit-base-patch32",
            use_safetensors=True,
            dtype=torch.float32
        ).to(device)

    experiments = []
    if not args.skip_text_only:
        experiments.append(("text_only", TextOnly(fresh_text(), 768)))
    if not args.skip_image_only:
        experiments.append(("image_only", ImageOnly(fresh_vision(), 768)))
    experiments.extend([
        ("concat",          ConcatFusion(fresh_text(), fresh_vision(), 768)),
        ("gmu",             GMUFusion(fresh_text(), fresh_vision(), 768)),
        ("cross_attention", CrossAttentionFusion(fresh_text(), fresh_vision(), 768, num_heads=8)),
    ])

    combined_results_json: Dict[str, Any] = {}
    combined_rows_csv: List[Dict[str, Any]] = []

    # Run experiments
    for model_key, model in experiments:
        print("\n" + "="*80)
        print(f"EXPERIMENT: {model_key}")
        print("="*80)

        # Train
        model = model.to(device)
        model, curves = train_model(
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            device=device,
            epochs=args.epochs,
            lr=args.lr,
            model_key=model_key
        )

        # Evaluate
        val_metrics, _, _, _ = evaluate_dl(model, val_loader, device)
        test_metrics, y_true, y_prob, y_pred = evaluate_dl(model, test_loader, device)

        # Export predictions
        export_preds(args.out_dir, model_key, y_true, y_prob, y_pred)

        # Embeddings (use trained backbones if available)
        t_backbone = getattr(model, "text", fresh_text())
        v_backbone = getattr(model, "vision", fresh_vision())
        for split_name, loader in [("train", train_loader), ("val", val_loader), ("test", test_loader)]:
            img_emb, txt_emb, lbls, hb, im = extract_simple_embeddings(loader, t_backbone, v_backbone, device)
            export_embeddings(args.out_dir, split_name, model_key, img_emb, txt_emb, lbls, hb, im)

        # Record results
        combined_results_json[model_key] = {
            "val": {
                "auc": val_metrics["auc"], "pr": val_metrics["pr"],
                "precision": val_metrics["precision"], "recall": val_metrics["recall"],
                "f1": val_metrics["f1"], "tp": val_metrics["tp"], "fp": val_metrics["fp"],
                "tn": val_metrics["tn"], "fn": val_metrics["fn"],
            },
            "test": {
                "auc": test_metrics["auc"], "pr": test_metrics["pr"],
                "precision": test_metrics["precision"], "recall": test_metrics["recall"],
                "f1": test_metrics["f1"], "tp": test_metrics["tp"], "fp": test_metrics["fp"],
                "tn": test_metrics["tn"], "fn": test_metrics["fn"],
            },
            "training_curve": curves
        }

        combined_rows_csv.append({
            "model_type": model_key,
            "val_auc": val_metrics["auc"],
            "val_pr": val_metrics["pr"],
            "val_precision": val_metrics["precision"],
            "val_recall": val_metrics["recall"],
            "val_f1": val_metrics["f1"],
            "test_auc": test_metrics["auc"],
            "test_pr": test_metrics["pr"],
            "test_precision": test_metrics["precision"],
            "test_recall": test_metrics["recall"],
            "test_f1": test_metrics["f1"],
            "tp": test_metrics["tp"],
            "fp": test_metrics["fp"],
            "tn": test_metrics["tn"],
            "fn": test_metrics["fn"],
        })

        # Save model weights
        torch.save(model.state_dict(), os.path.join(args.out_dir, f"{model_key}.pth"))

    # Write combined results
    with open(os.path.join(args.out_dir, "results.json"), "w") as f:
        json.dump(combined_results_json, f, indent=2)

    pd.DataFrame(combined_rows_csv).to_csv(os.path.join(args.out_dir, "results.csv"), index=False, float_format="%.6f")

    print("\n[+] All experiments completed. Combined results written to results.json and results.csv")

if __name__ == "__main__":
    main()